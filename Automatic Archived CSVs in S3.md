ğŸ‘‰ Do you want me to also enable automatic compression (Parquet format) for these archived CSVs in S3, to cut storage cost and improve Athena performance?

Perfect âœ… â€” letâ€™s upgrade the OU breakdown archive to use Parquet compression. This will:

ğŸ“‰ Reduce storage costs (Snappy-compressed Parquet is ~70â€“90% smaller than CSV).

âš¡ Speed up Athena queries (columnar format = only scans needed columns).

ğŸ” Still maintain versioning for audit compliance.

ğŸ”¹ 1. Adjust Lambda to Convert CSV â†’ Parquet

Weâ€™ll use pandas + pyarrow in the Lambda:

import boto3, os
import pandas as pd
from datetime import date

s3 = boto3.client("s3")

def archive_parquet(local_csv):
    today = date.today()
    bucket = os.environ["ARCHIVE_BUCKET"]
    key = f"ou-breakdowns/year={today.year}/month={today.month:02d}/day={today.day:02d}/ou_breakdown.parquet"

    # Load CSV â†’ Parquet
    df = pd.read_csv(local_csv)
    parquet_file = f"/tmp/ou_breakdown_{today}.parquet"
    df.to_parquet(parquet_file, engine="pyarrow", compression="snappy")

    # Upload Parquet
    s3.upload_file(parquet_file, bucket, key)
    print(f"Archived Parquet to s3://{bucket}/{key}")
    return f"s3://{bucket}/{key}"


Inside handler() replace:

archive_path = archive_csv(local_file)

with:

archive_path = archive_parquet(local_file)


ğŸ”¹ 2. Glue Crawler Adjustment

Crawler auto-detects Parquet schema. Update path if needed:

resource "aws_glue_crawler" "ou_breakdown" {
  name          = "finops-ou-breakdown-crawler"
  role          = aws_iam_role.glue_service_role.arn
  database_name = aws_glue_catalog_database.finops.name

  s3_target {
    path = "s3://finops-daily-summaries/ou-breakdowns/"
  }

  schedule = "cron(0 8 * * ? *)"
}


ğŸ”¹ 3. Athena Table Schema (Auto-Detected)

The Glue crawler creates:

CREATE EXTERNAL TABLE finops_db.ou_breakdown (
  BillingDate date,
  OU string,
  Service string,
  LinkedAccount string,
  DailyCost double
)
PARTITIONED BY (year int, month int, day int)
STORED AS PARQUET
LOCATION 's3://finops-daily-summaries/ou-breakdowns/';


ğŸ”¹ 4. Benefits

Cheaper storage: Parquet + Snappy compression slashes S3 cost.

Cheaper Athena queries: Athena charges per scanned data volume; Parquet cuts this drastically.

Future-proof: Many BI tools (QuickSight, PowerBI, Tableau) natively read Parquet.

ğŸ”¹ 5. End Result

ğŸ“§ FinOps team still gets daily digest email with CSV attachment.

ğŸ“‚ Archive in S3 = compressed Parquet files, partitioned by date.

ğŸ” Athena queries now scan less, cost less, run faster.

âœ… Versioning still enabled â†’ full audit trail preserved.

